{"/blog/2023-10-11-oracle-split-read":{"title":"Oracle大表的并行读取","data":{"测试环境#测试环境":"","硬件#硬件":"","数据#数据":"表结构\nCREATE TABLE TPCH.FP_RAW_DATA\n(\n    CP_IDX   number(20, 0)    not null,\n    WAFERID  varchar(10)      not null,\n    BIN      number(10, 0)    not null,\n    SITE     DOUBLE PRECISION,\n    X        DOUBLE PRECISION not null,\n    Y        DOUBLE PRECISION not null,\n    BIN_1    DOUBLE PRECISION not null,\n    BIN_2    DOUBLE PRECISION,\n    BIN_3    DOUBLE PRECISION,\n    -- BIN_4 ~ BIN_39 DOUBLE PRECISION\n    BIN_40   DOUBLE PRECISION,\n    BIN_CODE VARCHAR(10)\n);\nCREATE UNIQUE INDEX TPCH.FP_RAW_IDX2 ON TPCH.FP_RAW_DATA (CP_IDX, WAFERID, X, Y);\n准备20,000万（2亿）行数据\nSELECT TABLE_NAME\n     , NUM_ROWS\n     , AVG_ROW_LEN\n     , ROUND(AVG_ROW_LEN * NUM_ROWS / 1024 / 1024 / 1024, 2) AS GB\nFROM ALL_TABLES\nwhere OWNER = 'TPCH'\n  AND TABLE_NAME = 'FP_RAW_DATA_MARS3'\nTABLE_NAME\n--------------------------------------------------------------------------------\n  NUM_ROWS AVG_ROW_LEN\t       GB\n---------- ----------- ----------\nFP_RAW_DATA_MARS3\n 200000000\t    99\t    18.44\n分16个分片进行，平均每个分片处理12,500,000（1250万）行数据脚注","测试验证#测试验证":"编号\t切分字段\t切分方法\tA\tCP_IDX\tRange: (Minimum, Maximum)\tB\tROWID\tRange: (Minimum, Maximum)\tC\tROWID\tRange: Scan\tD\tROWID\tRange: DBMS_PARALLEL_EXECUTE.create_chunks_by_rowid\tE\tROWID\tHash","a-基于主键索引列按照min-max区间切分#A: 基于主键/索引列，按照(min, max)区间切分":"SELECT /*+parallel(4)*/\n    COUNT(1)       AS total\n     , MIN(CP_IDX) AS lower\n     , MAX(CP_IDX) AS upper\nFROM TPCH.FP_RAW_DATA;","b-基于rowid按照min-max区间切分#B: 基于ROWID，按照(min, max)区间切分":"SELECT /*+parallel(4)*/\n    COUNT(1)      AS total\n     , MIN(ROWID) AS lower\n     , MAX(ROWID) AS upper\nFROM TPCH.FP_RAW_DATA;","c-基于rowidscan-split-by-rows#C: 基于ROWID，Scan, split by rows":"SELECT /*+parallel(4)*/ id\nFROM (SELECT id, ROWNUM AS rn\n      FROM (SELECT ROWID AS id\n            FROM TPCH.FP_RAW_DATA\n            ORDER BY ROWID) t) x\nWHERE MOD(rn, 12500000) = 0","d-dbms_parallel_execute包create_chunks_by_rowid#D: DBMS_PARALLEL_EXECUTE包create_chunks_by_rowid":"BEGIN\n    DBMS_PARALLEL_EXECUTE.create_task(task_name => 'tpch_test');\n    DBMS_PARALLEL_EXECUTE.create_chunks_by_rowid(\n            task_name => 'tpch_test',\n            table_owner => 'TPCH',\n            table_name => 'FP_RAW_DATA',\n            by_row => TRUE,\n            chunk_size => 12500000);\nEND;\n/\nSELECT chunk_id, status, start_rowid, end_rowid\nFROM user_parallel_execute_chunks\nWHERE task_name = 'tpch_test'\nORDER BY chunk_id;","e-基于rowidhash切分#E: 基于ROWID，Hash切分":"SELECT *\nFROM TPCH.FP_RAW_DATA\nWHERE MOD(DBMS_ROWID.ROWID_ROW_NUMBER(TPCH.FP_RAW_DATA.ROWID), 16) = 0","结论#结论":":::tip\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum\ntempor eros aliquam consequat. Lorem ipsum dolor sit amet\n:::\n在我们的场景中，允许Partitioned Table?\n优先使用 DBMS_PARALLEL_EXECUTE包中的create_chunks_by_rowid方法，获取基于ROWID切分；\n如果因为版本兼容性或者权限等原因导致1失败，则尝试采用；\n啦啦啦\nOracle DBMS_PARALLEL_EXECUTEInfoSphere Information Server Partitioned read methods"}},"/blog/2023-12-29-v2023-released":{"title":"Release Notes (v2023)","data":{"":"2024年来临之际，我很荣幸地代表团队向大家宣布bluepipe2023年度版本的诞生。","背景#背景":"我们花费了几个月的时间构建出bluepipe第一个原型版本的时候，团队的成就感斐然。\n即便只有一个单机版的程序，在几个早期客户现场验证的时候，也不乏溢美之词，比如性能炸裂、靠谱、很智能等。但更值得我们关注的，却是客户用起来有点别扭、却没说出来甚至意识不到这是问题的地方。\n做产品调研最高效的方法，就是站在背后看用户怎么使用你的产品。我们通过远程演示、或者辅助用户跑通第一个作业（Aha moment），惭愧地看到，将后台程序的接口，包装成一个个表单直接展现给用户，\n是多么地懒惰和缺乏想象力。比如：\n以任务为操作客体，用户需要创建一个个任务来完成他的业务目标。这是一种面向过程的设计，用户需要非常理解底层的技术细节，才可能在实际使用中得心应手。\n操作层面，缺少场景封装。比如用户需要快速完成某张表从来源库重新同步（数据修复），那么他需要先停止CDC任务，再启动一个跑批的任务，然后再将CDC任务拉起来。步骤繁多且没有辅助校验，极易出错引发业务故障。\n出于以用户为中心，将麻烦留给自己的团队价值观，我们推出了此次版本更新。","主要变化#主要变化":"","全新交互#全新交互":"面向数据交付的全新交互，您只需要关注数据表的复制状态，而不用考虑怎么配置任务。","流批融合#流批融合":"优先使用CDC方式来持续地复制增量数据（需要来源库支持CDC能力）；\n对同一张数据表而言，基于SQL查询的批量复制与基于CDC技术的流式复制无缝衔接；\n一键封装重刷数据的功能；确保应急响应场景下，无需人工思考流批冲突的问题。","游标列#游标列":"支持为每张表单独设置时间类型的游标列以实现批量复制模式下的增量同步能力。\n:::caution 限制\n游标列本质上是一个业务字段，依赖您的应用程序正确地维护它。\n因此，如果某行数据被物理删除（DELETE），或者游标列没有被正确地更新，都有可能导致基于它的数据同步不能正确工作。此风险bluepipe无法识别，您需要谨慎评估使用此功能。\n:::","筛选与重命名#筛选与重命名":"数据复制到目标库时，支持表和列（字段）裁剪；\n数据复制到目标库时，支持表和列（字段）重命名。\n:::tip 提示\n表或者列重新进行筛选或者重命名不会影响正在运行或者运行结束的任务，新的配置会在任务下次启动时生效。\n您有可能您需要做完相关修改后人工进行数据重刷。\n:::","连接器#连接器":"","新增clickzetta#新增ClickZetta":"ClickZetta是云器科技研发的下一代Single-Engine为核心理念的湖仓平台。\n支持自动建表（Create Table）和修改表结构（Alter Table）；\n支持数据实时写入，支持基于Primary Key进行数据更新；\n暂未支持数据读取（含BATCH和CDC）。\n:::caution 限制\n受ClickZetta当前版本限制，对于含 Primary Key的表，目前仅支持单线程写入。bluepipe自动进行了适配，无Primary Key\n的表仍然可以多线程大吞吐写入。\n:::","新增intersystems#新增InterSystems":"InterSystems是一家提供数据管理和医疗信息系统的公司，为医疗健康、金融服务、供应链和物流等行业提供数据解决方案。\nInterSystems在2022 Gartner:copyright: 云数据库管理系统魔力象限:tm:中被评为“远见者（Visionary）”。\n兼容InterSystems IRIS 和InterSystems Caché两套引擎；\n支持通过批量方式读取数据；\n暂未支持基于日志订阅的 CDC 能力；\n暂未支持数据写入能力。","优化apache-hive#优化Apache Hive":"接入时支持Zookeeper Service Discovery配置；\nCDC数据以APPEND模式写入临时表，定期MERGE到主表。","遗留问题#遗留问题":"","错误率控制#错误率控制":"复制过程中的错误率控制尚未支持。\n因此，在当前版本中，如果某行数据因为字段类型不兼容、或者字符超长等原因而无法写入，则会导致整个任务失败。","cdc模式下表之间的耦合#CDC模式下表之间的耦合":"由于CDC技术的特殊性，出于数据库资源考虑，我们限制了一个作业最多启用一条CDC流来复制数据，这必然导致不同表之间互相耦合。\n即某张表无法写入，或者写入性能变慢，会导致同一个作业内其他表的复制也被阻塞。考虑解决此问题而带来的技术复杂度急剧上升，短期内我们不会着手去解决。您可基于业务优先级，将一个作业拆分成多个作业来规避这一问题。"}},"/blog/2024-01-19-v095-released":{"title":"Release Notes (v0.9.5)","data":{"主要变化#主要变化":"","支持通过定时器调度作业#支持通过定时器调度作业":"允许配置定时策略，以实现数据的自动复制；\n您可以在作业右上角查看定时任务的运行情况，支持操作开启、暂停定时任务。\n:::caution 注意\n定时器、外部 API 或者人工点击都可能触发作业运行，它们之间可能存在冲突，请您在操作时谨慎评估。\n:::","支持按schema筛选复制范围#支持按Schema筛选复制范围":"此版本将复制范围下放到数据库的Schema层级，为用户提供更灵活的选择。","调度策略优化#调度策略优化":"基于深度优先的作业调度方式，避免大量作业同时提交时排队时间过长。","切分策略优化#切分策略优化":"新增基于非 Number 列的切分支持，以提升复制过程中的并行度；\n优化切分列选择策略，规避数据倾斜。","连接器#连接器":"","新增sql-server#新增SQL Server":"支持通过批量方式读取数据；\n暂未支持基于日志订阅的 CDC 能力；\n暂未支持数据写入能力。"}},"/blog/2024-04-04-sqlplus-tips":{"title":"Tips for sqlplus","data":{"":"SET LINESIZE 100;\nSET PAGESIZE 100;\nSET timing on;\nset autotrace on;\nhttps://github.com/hanslub42/rlwrap\nset hist on;\nhistory 1 run;\nhist 2 edit;"}},"/blog/2024-06-03-v011-released":{"title":"Release Notes (v.0.11.0)","data":{"主要变化#主要变化":"","控制台#控制台":"作业配置下放到表层级；此变化可以帮助用户更精细地控制数据复制的范围；\n基于结果表的global search能力。","utility#Utility":"","启动项#启动项":"默认采用20%物理内存模式，以适应POC阶段广泛存在的与其他服务混合部署需求。","连接器#连接器":"","oracle-database改进#Oracle Database（改进）":"CDC：全面支持超过30个字符的表和字段；\n支持ROWID和XID附加列。"}},"/blog/2024-06-21-with-lakehouse":{"title":"构建跨网络数据管道最佳实践","data":{"":"本文以云器 Lakehouse 为例，介绍利用bluepipe构建跨网络的数据复制最佳实践。如下图，客户业务系统部署在私有 IDC 中，而Lakehouse作为多租户湖仓服务部署在云上。数据需要从Oracle同步到Lakehouse\n，我们推荐将bluepipe部署在客户 IDC 中，推数据到Lakehouse。","方案优势#方案优势":"","开箱即用10-分钟完成配置#开箱即用，10 分钟完成配置":"bluepipe几乎可以运行在任何Linux系统中，支持x86和arm芯片；常见的机架式服务器、笔记本电脑，甚至树莓派都可以用来部署；\n极简的配置过程，默认参数即可达到最佳性能。","全增量一体化无需运维干预#全、增量一体化，无需运维干预":"全量同步与增量同步深度协同，几乎无需日常运维操作；\n高效的数据比对与热修复技术，始终保证数据一致；\n高度鲁棒的Schema Evolution。","推数据而不是暴露端口#推数据，而不是暴露端口":"bluepipe与您的数据库一起部署在您的内网，无需对外暴露端口；\n弹性buffer size技术，在吞吐(Throughput)和延迟(latency)之间自动平衡。","oracle链路的独特优势#Oracle链路的独特优势":"bluepipe基于Oracle LogMiner实现对变更数据的捕捉。与此同时，在以下几个方面做了深度优化：","对ddl行为的深度兼容#对DDL行为的深度兼容":"在LogMiner默认策略下，当发生DDL行为后，相关表上后续的DML操作均无法正确解析，从而导致无法正确捕捉到变更。bluepipe维护了字典文件的自动构建策略，保证表结构变更后仍然能捕捉到正确的增量数据。","大事务优化#大事务优化":"Oracle Redo Log中记录了完整的事务(Transaction)过程，而业务上通常仅希望拿到commit\n之后的数据，因此，需要在传输过程中使用buffer来暂存尚未commit的变更记录。bluepipe基于独特的内存管理技术，单节点上也能轻松应对千万量级的大事务。","全面支持名称超过-30-字符的表#全面支持名称超过 30 字符的表":"Oracle 12.2版本之后，对表名、字段名的最大长度支持到了 128 字节。\n但因为种种原因，LogMiner对于无OGG LICENSE的实例，至今仍不支持对包含长名称的表的 DML\n解析，详情参考官方文档。bluepipe基于高效的流批融合技术，全面支持了此种情况下的增量数据捕捉与投递。","适配支持rac架构#适配支持RAC架构":""}},"/docs/concept/01-overview":{"title":"概述","data":{"拉拉#拉拉":":::note\n备注\n::::::tip\n提示\n::::::info\n信息\n::::::caution 警告\n警告 warning\n::::::danger 危险\n危险\n:::"}},"/docs/concept/02-principle":{"title":"核心理念","data":{"网络管理#网络管理":""}},"/docs/concept/03-tech-arch":{"title":"技术架构","data":{"云端管控#云端管控":"","边缘自治#边缘自治":""}},"/docs/concept/04-roadmap":{"title":"路线图","data":{"拉拉#拉拉":""}},"/docs/concept/05-why-not-others":{"title":"与开源方案比较","data":{"datax#DataX":"","flink-cdc#Flink CDC":""}},"/docs/concept/11-security/01-system-safety":{"title":"系统安全","data":{"网络与通信#网络与通信":"","我们如何保存密码#我们如何保存密码":"","保护您的数据库#保护您的数据库":""}},"/docs/concept/11-security/02-data-privacy":{"title":"数据隐私","data":{"数据在您的内网中流动#数据在您的内网中流动":"","pii数据与合规#PII数据与合规":""}},"/docs/concept/11-security":{"title":"安全体系","data":{}},"/docs/concept/12-scheduling/01-edge-cluster":{"title":"边缘集群","data":{}},"/docs/concept/12-scheduling/02-assignment":{"title":"网络分配","data":{"连通性检查#连通性检查":"","多地址#多地址":"","静态分配#静态分配":"","动态分配#动态分配":""}},"/docs/concept/12-scheduling":{"title":"如何调度作业","data":{}},"/docs/concept/12-scheduling/03-serverless":{"title":"弹性资源","data":{}},"/docs/concept/13-execution/01-job-plan":{"title":"执行计划","data":{"批流融合#批流融合":"","冲突检测#冲突检测":"","优先级#优先级":""}},"/docs/concept/13-execution/02-parallelism":{"title":"并行度与流控","data":{}},"/docs/concept/13-execution/03-idempotency":{"title":"幂等性与重试","data":{}},"/docs/concept/13-execution/04-verification":{"title":"一致性检查","data":{}},"/docs/concept/13-execution":{"title":"如何执行作业","data":{}},"/docs/concept/14-type-system/01-field-type":{"title":"字段类型","data":{}},"/docs/concept/14-type-system/02-schema-repair":{"title":"表结构迁移","data":{"表结构识别#表结构识别":"","类型迁移#类型迁移":"","限制与约束#限制与约束":""}},"/docs/concept/14-type-system":{"title":"类型系统","data":{}},"/docs/concept/14-type-system/03-limitation":{"title":"限制与约束","data":{}},"/docs/concept/15-connectors/01-sample-connector":{"title":"示例连接器","data":{"介绍#介绍":"","前置依赖#前置依赖":"","版本要求#版本要求":"5.7.*\n8.*.*\n查看当前 MySQL Server 版本的 SQL 语句为：\nSELECT VERSION();\n:::caution\n未列在上述清单里的版本，程序运行时不一定报错，但我们无法保证其每项功能都能正确工作。\n:::","系统参数#系统参数":"Variable Name\tValue\tRequired\tv1\tOFF\tYes\tv2\tOFF\tNo","服务状态#服务状态":"配置了主备复制的MySQL实例，如果从备库读取数据，其延迟不应该超过10分钟.查看主备复制延迟的命令为：\nSHOW REPLICA STATUS;\n:::tip\n威尔法我发\n:::","账号权限#账号权限":"","连接配置#连接配置":"","类型映射#类型映射":"","限制与约束#限制与约束":"","性能基准#性能基准":"","常见问题#常见问题":""}},"/docs/concept/15-connectors":{"title":"连接器","data":{}},"/docs/concept/16-integration/002-observability":{"title":"可观测性","data":{"grafana#Grafana":""}},"/docs/concept/16-integration/001-etl-tools":{"title":"ETL工具","data":{"azkaban#Azkaban":"","安装与配置#安装与配置":"","运行作业#运行作业":"","airflow#Airflow":"","安装与配置-1#安装与配置":"","运行作业-1#运行作业":"","dolphinscheduler#Dolphinscheduler":"","安装与配置-2#安装与配置":"","运行作业-2#运行作业":""}},"/docs/concept/16-integration":{"title":"与其他系统集成","data":{}},"/docs/faq":{"title":"FAQ","data":{}},"/docs/getting-started/11-deploy/01-docker":{"title":"采用 Docker 部署","data":{"":":::caution\nDocker部署仅用于进行产品原型验证，不能用于生产环境。\n:::","系统要求#系统要求":"","硬件资源#硬件资源":"x86_64 架构\nCPU: 4核\n内存: 16G\n硬盘: 100G","操作系统#操作系统":"CentOS 7.* 及以上","软件依赖#软件依赖":"Docker 24.* 及以上","网络要求#网络要求":"为了能够安装bluepipe运行所依赖的三方软件，您的机器应该能够访问互联网。\n为了达到本地验证的最佳效果，您的机器应该能够连接所需同步的数据源。","安装-docker#安装 Docker":"","通过yum安装#通过yum安装":"","增加yum源#增加yum源":"sudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo","安装#安装":"sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin","启动#启动":"sudo systemctl start docker","其他方式#其他方式":"更多安装详情请查看Docker官方文档\nInstall Docker Engine on CentOS","安装服务#安装服务":"","解压安装包#解压安装包":"在获取到Bluepipe的安装包可以看到两个tar.gz的压缩包，将images.tar.gz导入到docker镜像中，将另一个包解压。\n# 导入镜像\ndocker load < images.tar.gz\n# 解压docker代码包，压缩包格式是 bluepipe.tar.gz 或者 bluepipe-***.tar.gz\ntar -xf bluepipe-***.tar.gz","启动服务#启动服务":"进入解压后的docker目录，启动docker compose\ncd docker\ndocker compose up -d\n查看正在运行的docker容器\ndocker ps\nCONTAINER ID   IMAGE                        COMMAND                   CREATED         STATUS         PORTS                                              NAMES\n4edc4025c052   1stblue/openjdk-17:latest    \"/docker-entrypoint.…\"   5 seconds ago   Up 3 seconds                                                      bluepipe-worker-1\n7072bfed3ab8   1stblue/openjdk-17:latest    \"/docker-entrypoint.…\"   5 seconds ago   Up 3 seconds                                                      bluepipe-openapi-1\nda6786e3da14   openresty/openresty:alpine   \"/usr/local/openrest…\"   5 seconds ago   Up 4 seconds   0.0.0.0:443->443/tcp                               bluepipe-resty-1"}},"/docs/getting-started/11-deploy":{"title":"本地部署","data":{}},"/docs/getting-started":{"title":"快速上手 {#quickstart}","data":{"创建连接-creating-connection#创建连接 {#creating-connection}":"","创建作业-creating-jobs#创建作业 {#creating-jobs}":"","运行任务-run-tasks#运行任务 {#run-tasks}":"","验证数据-next-steps#验证数据 {#next-steps}":""}},"/docs/support/01-download":{"title":"配置与下载","data":{"sdk与cli#SDK与CLI":"我们提供了Python语言的SDK和CLI工具，您可通过下列地址随时获取最新版本：https://github.com/1stblue/sdk/releases/tag/latest"}},"/docs/support/03-promotion":{"title":"向团队介绍 BLUEPIPE","data":{"":"希望团队尽快用上BluePipe，不知道怎么介绍？别担心，以下是我们为您准备的资料。\n嵌入 video / PPT","下载#下载":"PPT 版\nKeynote 版\nLatex 版"}},"/docs/support/02-open-bug":{"title":"反馈 BUG","data":{"阿拉#阿拉":"我们采用github issue来管理 Bug。"}},"/docs/support/99-faq":{"title":"常见问题","data":{}},"/docs/tests/testing":{"title":"Testing","data":{"":"bluepipe's components and hooks are made from small pieces of code. Each component or hook is designed to be testable and work independently of each other.So, you don't need unit testing, because bluepipe is already tested by its maintainers. However, you can write unit tests in your own code (helper, definitions, etc.).We strongly recommend that you write end-to-end tests of your application. bluepipe used the cypress framework as an example. You are free to write tests with any framework you want.","example#Example":""}},"/blog/2024-04-23-v010-released":{"title":"Release Notes (v.0.10.1)","data":{"主要变化#主要变化":"","全新的连接设计#全新的连接设计":"JDBC URL的解析识别能力，提升Java开发者群体的使用体验；\n数据库流控能力，避免大量作业同时运行造成数据库不稳定；\n多地址能力，为后续高可用、读写分离功能做支撑。","工具集utility#工具集Utility":"docker网络诊断；\n作业配置备份功能。","连接器#连接器":"","postgressql改进#PostgresSQL（改进）":"CDC模式开启自动创建publication（需要super权限）；","clickzetta改进#ClickZetta（改进）":"DDL语句执行后强制刷新数据字典，显著降低作业失败率；","oracle-database改进#Oracle Database（改进）":"CDC模式下一系列bug fix；\nSchema Repair: integer类型识别成number的 bug fix；","gbase-8s新增#GBase 8s（新增）":"写入能力；","gaussdb新增#GaussDB（新增）":"写入能力；","基础镜像#基础镜像":"","runtime#runtime":"runtime是bluepipe的JDK运行环境，此次更新：\n新增krb5-user和xxd软件包，供Kerberos调试使用。","gateway#gateway":"gateway是bluepipe的openresty运行环境，负责用户请求的接入代理、身份认证和权限检查等工作。\n此次更新：\n独立出单独镜像；\n增加基于 password和标准OAuth身份认证功能。"}}}